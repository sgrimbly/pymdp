{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-Maze Demo\n",
    "\n",
    "In this notebook, we simulate a T-Maze task - a type of two-armed contextual bandit - using an active inference (AIF) agent with `pymdp`. Note that unlike earlier demos in the [legacy] version of pymdp, this demo relies on `pymdp`'s new backend in `jax`. The `jax` backend accelerates the [`pymdp` package](https://pymdp-rtd.readthedocs.io/en/latest/), originally written in `numpy`, by dispatching the core inference, learning and planning computations to CUDA-compatible GPU (if enabled). This, in combination with `jax`'s just-in-time (JIT) compilation features, enables pymdp to take advantage of batch processing (i.e., running many AIF processes in parallel) and increased memory-usage / speed.\n",
    "\n",
    "### The T-Maze Task\n",
    "\n",
    "The T-Maze task implemented in this notebook is adapted from [the sophisticated inference paper](https://discovery.ucl.ac.uk/id/eprint/10124606/), and was originally introduced in an active inference context in [\"Active Inference and Epistemic Value\"](). The T-maze is a two-armed contextual bandit: at any given time, the agent can choose between sampling a cue (context) or choosing between two reward arms (left vs. right).‚Äù This task represents a classic problem in sequential decision-making, where an agent (in this case, analogized to a rat) must navigate a T-shaped maze. The agent starts at the centre of the T-maze. Within either the left or right arm, there is either a preferred (i.e., rewarding; cheese) stimulus or an aversive (i.e., punishing; shock) stimulus, with these reward contingencies initially unknown to the agent. In the bottom part of the T-Maze, a cue provides information about the which arm the rewarding stimulus is in.\n",
    "\n",
    "The agent is faced with a dilemma: commit to one of the potentially rewarding arms or first seek information from the cue to identify the more rewarding option before taking action. We use the term \"cue validity\" to indicate the probability that the cue correctly indicates the reward's location. If the cue has information about which of the two arms is more rewarded (i.e., the cue validity is greater than 50%), then the optimal behavior entails first visiting the cue arm and then choosing one of the two reward arms. \n",
    "\n",
    "\n",
    "### Overview\n",
    "This notebook steps through the following:\n",
    "\n",
    "1. A deterministic generative process (environment), and a single agent solving the task with vanilla active inference.\n",
    "2. A noisy generative process, and a single agent solving the task with vanilla active inference.\n",
    "3. A noisy generative model with A and B learning, with correct and incorrect prior structure of those parameters, and a single agent solving the task with vanilla active inference.\n",
    "4. A deterministic generative process, and a single agent solving the task with sophisticated inference\n",
    "5. A deterministic generative process, and a single agent solving the task with inductive inference\n",
    "6. A deterministic generative process, and multiple agents solving the task with vanilla active inference\n",
    "7. A deterministic generative process, and multiple agents solving the task with sophisticated inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/conor/Documents/ml_at_verses/pymdp_dev/pymdp/pymdp/agent.py:651: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert (\n"
     ]
    }
   ],
   "source": [
    "# a way to edit and run code and see the effects in the notebook without having to restart the kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# importing necessary libraries\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mediapy\n",
    "\n",
    "from jax import random as jr\n",
    "from pymdp.envs import TMaze, rollout\n",
    "from pymdp.agent import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the T-Maze Task (Generative Process)\n",
    "\n",
    "- The `batch_size` parameter specifies the number of environments to run in parallel.\n",
    "- The `reward_condition` parameter determines the reward location: `0` for the left arm, `1` for the right arm, or `None` for random allocation.\n",
    "- The `reward_probability` parameter sets the chance of receiving a reward in the correct arm. For example, if set at 0.80, there would be an 80% chance of reward and 20% chance of no outcome in the rewarding arm.\n",
    "- The `punishment_probability` parameter specifies the likelihood of punishment in the other arm. For example, if set at 0.80, there would be an 80% chance of punishment and 20% chance of no outcome in the non-rewarding arm.\n",
    "- The `cue_validity` parameter represents the accuracy of the cues as a probability between 0 and 1.\n",
    "\n",
    "<details>\n",
    "<summary> Click here to see how the generative process is set up. </summary>\n",
    "\n",
    "#### States and Observations\n",
    "\n",
    "**State Factors:**\n",
    "1. Location (5 states):\n",
    "    - 0: centre (start location)\n",
    "    - 1: left arm\n",
    "    - 2: right arm\n",
    "    - 3: cue location (bottom arm)\n",
    "    - 4: middle of arms (between left and right arm)\n",
    "2. Reward Location (2 states):\n",
    "    - 0: reward in left arm \n",
    "    - 1: reward in right arm\n",
    "\n",
    "**Observation Modalities:**\n",
    "1. Location (5 observations):\n",
    "    - Matches the location states exactly\n",
    "2. Outcome (3 observations):\n",
    "    - 0: no outcome\n",
    "    - 1: reward (cheese)\n",
    "    - 2: punishment (shock)\n",
    "3. Cue (3 observations):\n",
    "    - 0: no cue\n",
    "    - 1: left arm cued\n",
    "    - 2: right arm cued\n",
    "\n",
    "#### Environment Parameters\n",
    "\n",
    "**Observation Likelihood Model (A):**\n",
    "- A[0]: Location observations (5x5 tensor)\n",
    "  - Perfect mapping between true and observed location.\n",
    "- A[1]: Outcome observations (3x5x2 tensor)\n",
    "  - In the more rewarding arm, reward is presented with a likelihood determined by the `reward_probability` parameter.\n",
    "  - In the less rewarding arm, punishment is presented with a likelihood determined by the `punishment_probability` parameter.\n",
    "  - No outcomes are observed in the centre/start location, cue location, or middle of the arms.\n",
    "- A[2]: Cue observations (3x5x2 tensor)\n",
    "  - Indicating the reward location, at the cue location (bottom arm), with accuracy set by the `cue_validity` parameter.\n",
    "  - No cues visible elsewhere.\n",
    "\n",
    "**Transition Model (B):**\n",
    "- B[0]: Location transitions (5x5x5 tensor)\n",
    "  - Agent can move between adjacent maze cells or stay in the same cell.\n",
    "- B[1]: Reward location (2x2x1 tensor)\n",
    "  - Reward location remains fixed throughout trial.\n",
    "\n",
    "**Initial Conditions (D):**\n",
    "- D[0]: Starting location (5x1 tensor)\n",
    "  - Agent always begins in centre location\n",
    "- D[1]: Reward placement (2x1 tensor)\n",
    "  - Default: Equal chance (50/50) of reward in either arm (`reward_condition=None`)\n",
    "  - Optional: Can fix reward to specific arm, by setting `reward_condition` to `0` (for left arm) or `1` (for right arm)\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the parameters for the environment\n",
    "batch_size = 1 # batch_size, which in this case corresponds to the number of environments to run in parallel\n",
    "reward_condition = None # 0 is reward in left arm, 1 is reward in right arm, None is random allocation\n",
    "reward_probability = 1.0 # 100% chance of reward in the correct arm\n",
    "punishment_probability = 1.0 # 100% chance of punishment in the other arm\n",
    "cue_validity = 1.0 # 100% valid cues\n",
    "dependent_outcomes = False # if True, punishment occurs as a function of reward probability (i.e., if reward probability is 0.8, then 20% punishment). If False, punishment occurs with set probability (i.e., 20% no outcome and punishment will only occur in the other (non-rewarding) arm)\n",
    "\n",
    "# initialising the environment. see tmaze.py in pymdp/envs for the implementation details.\n",
    "env = TMaze( \n",
    "    batch_size=batch_size, \n",
    "    reward_probability=reward_probability,     \n",
    "    punishment_probability=punishment_probability, \n",
    "    cue_validity=cue_validity,          \n",
    "    reward_condition=reward_condition,\n",
    "    dependent_outcomes=dependent_outcomes\n",
    ")\n",
    "\n",
    "# you may print the environment parameters to see the shapes of the tensors and the values by editing and uncommenting the following lines and running the code: \n",
    "\n",
    "# print([a.shape for a in env.params[\"A\"]]) # shape of all A tensors; the shape should start with the batch_size, then the rows, columns, and additional dimensions for the dependencies\n",
    "# print(env.params[\"A\"][1][0][:,:,1]) # likelihood of observing no outcome, reward, or punishment (rows), in each location (columns), when the reward condition is 1 (right arm)\n",
    "# print(env.params[\"A\"][2][0][:,:,0]) # likelihood of observing no cue, left arm cued, or right arm cued (rows), in each location (columns), when the reward condition is 0 (left arm)\n",
    "\n",
    "# print([b.shape for b in env.params[\"B\"]]) # shape of all B tensors\n",
    "# print(env.params[\"B\"][0][0][:,:,4]) # probability of transitioning to each location (rows), from each location (columns), when the agent wants to move to the middle of the arms (location 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. A deterministic generative process (environment), and a single agent solving the task with vanilla active inference.\n",
    "\n",
    "### Creating the Agent (Generative Model)\n",
    "\n",
    "We will create the agent's generative model based off the A and B tensors of the environment. The A and B tensors remain the same as, in this simple design, we assume the agent has knowledge of the environment's parameters - i.e., it knows that the likelihood of observing a reward in the left arm is 1.0 (`reward_probability=1.0`) if the reward is actually in the left arm (`reward_condition=0`), and it knows the cues are 100% accurate (`cue_validity=1.0`), and it knows that the reward location will be fixed throughout the trial (non-volatile environment). We can of course change these assumptions to create a more complex agent, and we will do that in the next sections where the environment will be more stochastic and we will also add uncertainty to the agent's generative model so it will have to learn that the environment is deterministic or not.\n",
    "\n",
    "The preference tensors (C) are set using the A tensor's shape. The agent is set to prefer reward and avoid punishment. The agent is set to not have any preference to observe certain locations and cues. \n",
    "\n",
    "The initial beliefs tensors (D; i.e., priors) are set using the B tensor's shape. The agent has a prior to start in the center location and it has no prior about the reward location - i.e., the prior for the reward location is uniformly distributed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  setting A tensors from the environment parameters\n",
    "A = [jnp.array(a, dtype=jnp.float32) for a in env.params[\"A\"]]\n",
    "A_dependencies = env.dependencies[\"A\"] # dependencies allow you to specify which state factors each observation modality depends on, so you dont have to store all the conditional dependencies between all state factors and each modality\n",
    "\n",
    "# setting B tensors from the environment parameters\n",
    "B = [jnp.array(b, dtype=jnp.float32) for b in env.params[\"B\"]]\n",
    "B_dependencies = env.dependencies[\"B\"]\n",
    "\n",
    "# creating C tensors filled with zeros for [location], [reward], [cue] based on A shapes\n",
    "C = [jnp.zeros((batch_size, a.shape[1]), dtype=jnp.float32) for a in A] \n",
    "# setting preferences for outcomes only\n",
    "C[1] = C[1].at[:,1].set(2.0)    # prefer reward\n",
    "C[1] = C[1].at[:,2].set(-3.0)   # avoid punishment\n",
    "\n",
    "\n",
    "# creating D tensors [location], [reward] based on B shapes\n",
    "D = []\n",
    "# D[0]: location - all zeros except location 0 (centre) because the agent always starts in the centre\n",
    "D_loc = jnp.zeros((batch_size, B[0].shape[1]), dtype=jnp.float32) \n",
    "D_loc = D_loc.at[0,0].set(1.0)  # set centre location to 1.0\n",
    "D.append(D_loc)\n",
    "\n",
    "# D[1]: reward location - uniform distribution\n",
    "D_reward = jnp.ones((batch_size, B[1].shape[1]), dtype=jnp.float32) \n",
    "D_reward = D_reward / jnp.sum(D_reward, axis=1, keepdims=True)  # normalise to get uniform distribution\n",
    "D.append(D_reward)\n",
    "\n",
    "\n",
    "# initialising the agent\n",
    "agent = Agent(\n",
    "    A, B, C, D, \n",
    "    policy_len=2, # how long the action sequence is that the agent is evaluating\n",
    "    A_dependencies=A_dependencies, \n",
    "    B_dependencies=B_dependencies,\n",
    "    apply_batch=False,\n",
    "    learn_A=False,\n",
    "    learn_B=False\n",
    ")\n",
    "\n",
    "# you may print the agent's generative model parameters to see the shapes of the tensors and the values by editing and uncommenting the following lines and running the code: \n",
    "\n",
    "# print([a.shape for a in agent.A]) # shape of all A tensors\n",
    "# print(agent.A[1][0][:,:,1]) # likelihood of observing no outcome, reward, or punishment (rows), in each location (columns), when the reward condition is 1 (right arm)\n",
    "# print(agent.C[1]) # preferences for outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the active inference agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jr.PRNGKey(0) # random key for the aif loop\n",
    "T = 10 # number of timesteps to rollout the aif loop for\n",
    "_, info, _ = rollout(agent, env, num_timesteps=T, rng_key=key) # running the aif loop\n",
    "\n",
    "# you may print the info dictionary to see the numerical results of the aif agent completing the T-maze task by editing and uncommenting the following lines and running the code: \n",
    "\n",
    "# print(info.keys()) # keys in the info dictionary\n",
    "# print(info[\"action\"][:,0,:]) # actions taken by the agent (locations throughout the maze) \n",
    "# print(info[\"observation\"][0]) # observations of the locations for each batch\n",
    "# print(info[\"observation\"][2]) # observations of the cues for each batch\n",
    "# print(jnp.around(info[\"qs\"][1], decimals=2)) # posterior beliefs about the reward location\n",
    "# print(info[\"qpi\"][0].shape) # shape of the policy tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rendering the Task to Visualise the Agent's Behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "for t in range(info[\"observation\"][0].shape[0]):  # iterate over timesteps\n",
    "    # get observations for this timestep\n",
    "    observations_t = [\n",
    "        info[\"observation\"][0][t, :, :],\n",
    "        info[\"observation\"][1][t, :, :],  \n",
    "        info[\"observation\"][2][t, :, :]   \n",
    "    ]\n",
    "       \n",
    "    frame = env.render(mode=\"rgb_array\", observations=observations_t) # render the environment using the observations for this timestep\n",
    "    frame = np.asarray(frame, dtype=np.uint8)\n",
    "    plt.close()  # close the figure to prevent memory leak\n",
    "    frames.append(frame)\n",
    "\n",
    "frames = np.array(frames, dtype=np.uint8)\n",
    "mediapy.show_video(frames, fps=1)\n",
    "\n",
    "# # uncomment the following lines to save the video as a gif\n",
    "# os.makedirs(\"figures\", exist_ok=True)\n",
    "# pil_frames = [Image.fromarray(frame) for frame in frames]\n",
    "# reward_location = \"random\" if reward_condition is None else (\"left\" if reward_condition == 0 else \"right\")\n",
    "# filename = os.path.join(\"figures\", f\"tmaze_{batch_size}_{reward_location}.gif\")\n",
    "# pil_frames[0].save(\n",
    "#     filename,\n",
    "#     save_all=True,\n",
    "#     append_images=pil_frames[1:],\n",
    "#     duration=1000,  # 1000ms per frame\n",
    "#     loop=0\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run multiple agents in parallel to see how they solve the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 9 # number of environments to run in parallel\n",
    "\n",
    "# initialising the environment\n",
    "env = TMaze( \n",
    "    batch_size=batch_size, \n",
    "    reward_probability=reward_probability,     \n",
    "    punishment_probability=punishment_probability, \n",
    "    cue_validity=cue_validity,          \n",
    "    reward_condition=reward_condition,\n",
    "    dependent_outcomes=dependent_outcomes\n",
    ")\n",
    "\n",
    "# initialising the agent's generative model - we need to generate this again to be the same size as the environment's batch_size. \n",
    "#  setting A tensors from the environment parameters\n",
    "A = [jnp.array(a, dtype=jnp.float32) for a in env.params[\"A\"]]\n",
    "A_dependencies = env.dependencies[\"A\"] # dependencies allow you to specify the state factors the observation modality depends on so you dont have to compute the full tensor using all state factors\n",
    "\n",
    "# setting B tensors from the environment parameters\n",
    "B = [jnp.array(b, dtype=jnp.float32) for b in env.params[\"B\"]]\n",
    "B_dependencies = env.dependencies[\"B\"]\n",
    "\n",
    "# creating C tensors filled with zeros for [location], [reward], [cue] based on A shapes\n",
    "C = [jnp.zeros((batch_size, a.shape[1]), dtype=jnp.float32) for a in A] \n",
    "# setting preferences for outcomes only\n",
    "C[1] = C[1].at[:,1].set(2.0)    # prefer reward\n",
    "C[1] = C[1].at[:,2].set(-3.0)   # avoid punishment\n",
    "\n",
    "\n",
    "# creating D tensors [location], [reward] based on B shapes\n",
    "D = []\n",
    "# D[0]: location - all zeros except location 0 (centre) because the agent always starts in the centre\n",
    "D_loc = jnp.zeros((batch_size, B[0].shape[1]), dtype=jnp.float32) \n",
    "D_loc = D_loc.at[0,0].set(1.0)  # set centre location to 1.0\n",
    "D.append(D_loc)\n",
    "\n",
    "# D[1]: reward location - uniform distribution\n",
    "D_reward = jnp.ones((batch_size, B[1].shape[1]), dtype=jnp.float32) \n",
    "D_reward = D_reward / jnp.sum(D_reward, axis=1, keepdims=True)  # normalise to get uniform distribution\n",
    "D.append(D_reward)\n",
    "\n",
    "\n",
    "# initialising the agent\n",
    "agent = Agent(\n",
    "    A, B, C, D, \n",
    "    policy_len=2,\n",
    "    A_dependencies=A_dependencies, \n",
    "    B_dependencies=B_dependencies,\n",
    "    apply_batch=False,\n",
    "    learn_A=False,\n",
    "    learn_B=False\n",
    ")\n",
    "\n",
    "_, info, _ = rollout(agent, env, num_timesteps=T, rng_key=key) # running the aif loop\n",
    "\n",
    "# rendering the task to visualise the agent's behaviour \n",
    "frames = []\n",
    "for t in range(info[\"observation\"][0].shape[0]):  # iterate over timesteps\n",
    "    # get observations for this timestep\n",
    "    observations_t = [\n",
    "        info[\"observation\"][0][t, :, :],\n",
    "        info[\"observation\"][1][t, :, :],  \n",
    "        info[\"observation\"][2][t, :, :]   \n",
    "    ]\n",
    "       \n",
    "    frame = env.render(mode=\"rgb_array\", observations=observations_t) # render the environment using the observations for this timestep\n",
    "    frame = np.asarray(frame, dtype=np.uint8)\n",
    "    plt.close()  # close the figure to prevent memory leak\n",
    "    frames.append(frame)\n",
    "\n",
    "frames = np.array(frames, dtype=np.uint8)\n",
    "mediapy.show_video(frames, fps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. A noisy generative process, and a single agent solving the task with vanilla active inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE GENERATIVE PROCESS (NOISY)\n",
    "# setting the parameters for the environment\n",
    "batch_size = 4 # number of environments to run in parallel\n",
    "reward_condition = None # 0 is reward in left arm, 1 is reward in right arm, None is random allocation\n",
    "reward_probability = 0.7 # 70% chance of reward in the correct arm\n",
    "punishment_probability = 0.6 # 60% chance of punishment in the other arm\n",
    "cue_validity = 0.9 # 90% valid cues\n",
    "dependent_outcomes = False # if True, punishment occurs as a function of reward probability (i.e., if reward probability is 0.8, then 20% punishment). If False, punishment occurs with set probability (i.e., 20% no outcome and punishment will only occur in the other (non-rewarding) arm)\n",
    "\n",
    "# initialising the environment. see tmaze.py in pymdp/envs for the implementation details.\n",
    "env = TMaze( \n",
    "    batch_size=batch_size, \n",
    "    reward_probability=reward_probability,     \n",
    "    punishment_probability=punishment_probability, \n",
    "    cue_validity=cue_validity,          \n",
    "    reward_condition=reward_condition,\n",
    "    dependent_outcomes=dependent_outcomes\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE GENERATIVE MODEL\n",
    "#  setting A tensors from the environment parameters\n",
    "A = [jnp.array(a, dtype=jnp.float32) for a in env.params[\"A\"]]\n",
    "A_dependencies = env.dependencies[\"A\"] # dependencies allow you to specify the state factors the observation modality depends on so you dont have to compute the full tensor using all state factors\n",
    "\n",
    "# setting B tensors from the environment parameters\n",
    "B = [jnp.array(b, dtype=jnp.float32) for b in env.params[\"B\"]]\n",
    "B_dependencies = env.dependencies[\"B\"]\n",
    "\n",
    "# creating C tensors filled with zeros for [location], [reward], [cue] based on A shapes\n",
    "C = [jnp.zeros((batch_size, a.shape[1]), dtype=jnp.float32) for a in A] \n",
    "# setting preferences for outcomes only\n",
    "C[1] = C[1].at[:,1].set(3.0)    # prefer reward\n",
    "C[1] = C[1].at[:,2].set(-3.0)   # avoid punishment\n",
    "\n",
    "\n",
    "# creating D tensors [location], [reward] based on B shapes\n",
    "D = []\n",
    "# D[0]: location - all zeros except location 0 (centre) because the agent always starts in the centre\n",
    "D_loc = jnp.zeros((batch_size, B[0].shape[1]), dtype=jnp.float32) \n",
    "D_loc = D_loc.at[0,0].set(1.0)  # set centre location to 1.0\n",
    "D.append(D_loc)\n",
    "\n",
    "# D[1]: reward location - uniform distribution\n",
    "D_reward = jnp.ones((batch_size, B[1].shape[1]), dtype=jnp.float32) \n",
    "D_reward = D_reward / jnp.sum(D_reward, axis=1, keepdims=True)  # normalise to get uniform distribution\n",
    "D.append(D_reward)\n",
    "\n",
    "\n",
    "# initialising the agent\n",
    "agent = Agent(\n",
    "    A, B, C, D, \n",
    "    policy_len=3, # how long the action sequence is that the agent is evaluating\n",
    "    A_dependencies=A_dependencies, \n",
    "    B_dependencies=B_dependencies,\n",
    "    apply_batch=False,\n",
    "    learn_A=False,\n",
    "    learn_B=False\n",
    ")\n",
    "\n",
    "key = jr.PRNGKey(0) # random key for the aif loop\n",
    "T = 20 # number of timesteps to rollout the aif loop for\n",
    "_, info, _ = rollout(agent, env, num_timesteps=T, rng_key=key) # running the aif loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "for t in range(info[\"observation\"][0].shape[0]):  # iterate over timesteps\n",
    "    # get observations for this timestep\n",
    "    observations_t = [\n",
    "        info[\"observation\"][0][t, :, :],\n",
    "        info[\"observation\"][1][t, :, :],  \n",
    "        info[\"observation\"][2][t, :, :]   \n",
    "    ]\n",
    "       \n",
    "    frame = env.render(mode=\"rgb_array\", observations=observations_t) # render the environment using the observations for this timestep\n",
    "    frame = np.asarray(frame, dtype=np.uint8)\n",
    "    plt.close()  # close the figure to prevent memory leak\n",
    "    frames.append(frame)\n",
    "\n",
    "frames = np.array(frames, dtype=np.uint8)\n",
    "mediapy.show_video(frames, fps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. A noisy generative model with A and B learning, with correct and incorrect prior structure of those parameters, and a single agent solving the task with vanilla active inference.\n",
    "\n",
    "Here, you may tweak the parameters of learn_A and learn_B between True and False when initialising the agent to conduct A learning only, B learning only, or A and B learning together. First, we will set a correct prior structure of the A and B tensors. Then, we will set an incorrect prior structure of the A and B tensors. \n",
    "\n",
    "### Correct Prior Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the parameters for the environment\n",
    "batch_size = 1 # number of environments to run in parallel\n",
    "reward_condition = 0 # 0 is reward in left arm, 1 is reward in right arm, None is random allocation\n",
    "reward_probability = 1.0 # 100% chance of reward in the correct arm\n",
    "punishment_probability = 1.0 # 100% chance of punishment in the other arm\n",
    "cue_validity = 1.0 # 100% valid cues\n",
    "dependent_outcomes = False\n",
    "\n",
    "# initialising the environment. see tmaze.py in pymdp/envs for the implementation details.\n",
    "env = TMaze( \n",
    "    batch_size=batch_size, \n",
    "    reward_probability=reward_probability,     \n",
    "    punishment_probability=punishment_probability, \n",
    "    cue_validity=cue_validity,          \n",
    "    reward_condition=reward_condition, \n",
    "    dependent_outcomes=dependent_outcomes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the agent's generative model noisy. Borrowing the structure of the tensors from the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  setting A tensors from the environment parameters\n",
    "A = [jnp.array(a, dtype=jnp.float32) for a in env.params[\"A\"]]\n",
    "A_dependencies = env.dependencies[\"A\"] \n",
    "\n",
    "# # adding noise to flatten the distributions (make more uncertain) \n",
    "noise_level = 0.3 \n",
    "for i in [1, 2]:  # only modifying the outcome (i=1) and cue (i=2) observation likelihood mappings\n",
    "    # A[i] = jnp.flip(A[i], axis=1) # flipping for testing purposes\n",
    "\n",
    "    noise = noise_level * jnp.ones_like(A[i])\n",
    "    A[i] = A[i] + noise\n",
    "    A[i] = A[i] / jnp.sum(A[i], axis=1, keepdims=True) # normalise to ensure each distribution sums to 1\n",
    "\n",
    "pA = A\n",
    "\n",
    "# setting B tensors from the environment parameters\n",
    "B = [jnp.array(b, dtype=jnp.float32) for b in env.params[\"B\"]]\n",
    "B_dependencies = env.dependencies[\"B\"]\n",
    "\n",
    "# B[0] = jnp.flip(B[0], axis=(1,2)) # flipping for testing purposes\n",
    "\n",
    "# adding noise to flatten the distributions (make more uncertain) \n",
    "# key, subkey = jr.split(key)\n",
    "# noise = noise_level * jr.uniform(subkey, shape=B[0].shape)\n",
    "# B[0] = B[0] + noise\n",
    "# B[0] = B[0] / jnp.sum(B[0], axis=1, keepdims=True) # normalise to ensure each distribution sums to 1\n",
    "\n",
    "key, subkey = jr.split(key)\n",
    "noise = noise_level * jr.uniform(subkey, shape=B[1].shape)\n",
    "# B[1] = jnp.flip(B[1], axis=(1,2)) # flipping for testing purposes\n",
    "B[1] = B[1] + noise\n",
    "B[1] = B[1] / jnp.sum(B[1], axis=1, keepdims=True) # normalise to ensure each distribution sums to 1\n",
    "\n",
    "pB = B\n",
    "\n",
    "# creating C tensors filled with zeros for [location], [reward], [cue] based on A shapes\n",
    "C = [jnp.zeros((batch_size, a.shape[1]), dtype=jnp.float32) for a in A] \n",
    "# setting preferences for outcomes only\n",
    "C[1] = C[1].at[:,1].set(2.0)    # prefer reward\n",
    "C[1] = C[1].at[:,2].set(-3.0)   # avoid punishment\n",
    "\n",
    "\n",
    "# creating D tensors [location], [reward] based on B shapes\n",
    "D = []\n",
    "# D[0]: location - all zeros except location 0 (centre) because the agent always starts in the centre\n",
    "D_loc = jnp.zeros((batch_size, B[0].shape[1]), dtype=jnp.float32) \n",
    "D_loc = D_loc.at[0,0].set(1.0)  # set centre location to 1.0\n",
    "D.append(D_loc)\n",
    "\n",
    "# D[1]: reward location - uniform distribution\n",
    "D_reward = jnp.ones((batch_size, B[1].shape[1]), dtype=jnp.float32) \n",
    "D_reward = D_reward / jnp.sum(D_reward, axis=1, keepdims=True)  # normalise to get uniform distribution\n",
    "D.append(D_reward)\n",
    "\n",
    "\n",
    "# initialising the agent\n",
    "agent = Agent(\n",
    "    A, B, C, D, \n",
    "    pA=pA,\n",
    "    pB=pB, # adding the noisy A tensor for learning\n",
    "    policy_len=5, # how long the action sequence is that the agent is evaluating\n",
    "    A_dependencies=A_dependencies, \n",
    "    B_dependencies=B_dependencies,\n",
    "    apply_batch=False, \n",
    "    learn_A=False,\n",
    "    learn_B=True,\n",
    "    gamma=0.1,\n",
    "    action_selection=\"stochastic\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jr.PRNGKey(0) # random key for the aif loop\n",
    "T = 10 # number of timesteps to rollout the aif loop for\n",
    "_, info, _ = rollout(agent, env, num_timesteps=T, rng_key=key) # running the aif loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By printing the following statements out, we can see that the agent's A and B tensors are learning or updating from the first timestep to the last timestep to be closer to the environment's A and B tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"the environment's A tensor\")\n",
    "print(env.params[\"A\"][1][0][:,:,1])\n",
    "print()\n",
    "print(\"the agent's A tensor at t=0\")\n",
    "print(agent.A[1][0][:,:,1])\n",
    "print()\n",
    "print(f\"the agent's A tensor at t={T}\")\n",
    "print(info[\"agent\"].A[1][-1,0,:,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(info[\"agent\"].B[0][0,0,:,:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"the environment's B tensor\")\n",
    "print(env.params[\"B\"][1][0][:,:,1])\n",
    "print()\n",
    "print(\"the agent's B tensor at t=0\")\n",
    "print(info[\"agent\"].B[1][0,0,:,:,1])\n",
    "print()\n",
    "print(f\"the agent's B tensor at t={T}\")\n",
    "print(info[\"agent\"].B[1][-1,0,:,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "for t in range(info[\"observation\"][0].shape[0]):  # iterate over timesteps\n",
    "    # get observations for this timestep\n",
    "    observations_t = [\n",
    "        info[\"observation\"][0][t, :, :],\n",
    "        info[\"observation\"][1][t, :, :],  \n",
    "        info[\"observation\"][2][t, :, :]   \n",
    "    ]\n",
    "       \n",
    "    frame = env.render(mode=\"rgb_array\", observations=observations_t) # render the environment using the observations for this timestep\n",
    "    frame = np.asarray(frame, dtype=np.uint8)\n",
    "    plt.close()  # close the figure to prevent memory leak\n",
    "    frames.append(frame)\n",
    "\n",
    "frames = np.array(frames, dtype=np.uint8)\n",
    "mediapy.show_video(frames, fps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random initialization test\n",
    "# key = jr.PRNGKey(42)\n",
    "key = jr.PRNGKey(24)\n",
    "\n",
    "#  setting A tensors from the environment parameters\n",
    "A = [jnp.array(a, dtype=jnp.float32) for a in env.params[\"A\"]]\n",
    "A_dependencies = env.dependencies[\"A\"] \n",
    "\n",
    "# adding noise to flatten the distributions (make more uncertain) \n",
    "noise_level = 0.3 \n",
    "for i in [1, 2]:  # only modifying the outcome (i=1) and cue (i=2) observation likelihood mappings\n",
    "    # A[i] = jnp.flip(A[i], axis=1) # flipping for testing purposes\n",
    "\n",
    "    key, subkey = jr.split(key)\n",
    "    A[i] = jr.uniform(subkey, shape=A[i].shape) # generating random values between 0 and 1\n",
    "\n",
    "    A[i] = A[i] / jnp.sum(A[i], axis=1, keepdims=True) # normalise to ensure each distribution sums to 1\n",
    "\n",
    "pA = A\n",
    "\n",
    "\n",
    "\n",
    "# setting B tensors from the environment parameters\n",
    "B = [jnp.array(b, dtype=jnp.float32) for b in env.params[\"B\"]]\n",
    "B_dependencies = env.dependencies[\"B\"]\n",
    "\n",
    "# B[0] = jnp.flip(B[0], axis=(1,2)) # flipping for testing purposes\n",
    "\n",
    "# adding noise to flatten the distributions (make more uncertain) \n",
    "key, subkey = jr.split(key)\n",
    "B[0] = jr.uniform(subkey, shape=B[0].shape)\n",
    "B[0] = B[0] / jnp.sum(B[0], axis=1, keepdims=True) # normalise to ensure each distribution sums to 1\n",
    "\n",
    "key, subkey = jr.split(key)\n",
    "B[1] = jr.uniform(subkey, shape=B[1].shape)\n",
    "B[1] = B[1] / jnp.sum(B[1], axis=1, keepdims=True) # normalise to ensure each distribution sums to 1\n",
    "\n",
    "pB = B\n",
    "\n",
    "\n",
    "# initialising the agent\n",
    "agent = Agent(\n",
    "    A, B, C, D, \n",
    "    pA=pA,\n",
    "    pB=pB, # adding the noisy A tensor for learning\n",
    "    policy_len=5, # how long the action sequence is that the agent is evaluating\n",
    "    A_dependencies=A_dependencies, \n",
    "    B_dependencies=B_dependencies,\n",
    "    apply_batch=False, \n",
    "    learn_A=True,\n",
    "    learn_B=True,\n",
    "    gamma=0.1,\n",
    "    action_selection=\"stochastic\"\n",
    ")\n",
    "\n",
    "# running the active inference simulation\n",
    "key = jr.PRNGKey(0) # random key for the aif loop\n",
    "T = 50 # number of timesteps to rollout the aif loop for\n",
    "_, info, _ = rollout(agent, env, num_timesteps=T, rng_key=key) # running the aif loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"the environment's A tensor\")\n",
    "print(env.params[\"A\"][1][0][:,:,1])\n",
    "print()\n",
    "print(\"the agent's A tensor at t=0\")\n",
    "print(agent.A[1][0][:,:,1])\n",
    "print()\n",
    "print(f\"the agent's A tensor at t={T}\")\n",
    "print(info[\"agent\"].A[1][-1,0,:,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"the environment's B tensor\")\n",
    "print(env.params[\"B\"][1][0][:,:,1])\n",
    "print()\n",
    "print(\"the agent's B tensor at t=0\")\n",
    "print(info[\"agent\"].B[1][0,0,:,:,1])\n",
    "print()\n",
    "print(f\"the agent's B tensor at t={T}\")\n",
    "print(info[\"agent\"].B[1][-1,0,:,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "for t in range(info[\"observation\"][0].shape[0]):  # iterate over timesteps\n",
    "    # get observations for this timestep\n",
    "    observations_t = [\n",
    "        info[\"observation\"][0][t, :, :],\n",
    "        info[\"observation\"][1][t, :, :],  \n",
    "        info[\"observation\"][2][t, :, :]   \n",
    "    ]\n",
    "       \n",
    "    frame = env.render(mode=\"rgb_array\", observations=observations_t) # render the environment using the observations for this timestep\n",
    "    frame = np.asarray(frame, dtype=np.uint8)\n",
    "    plt.close()  # close the figure to prevent memory leak\n",
    "    frames.append(frame)\n",
    "\n",
    "frames = np.array(frames, dtype=np.uint8)\n",
    "mediapy.show_video(frames, fps=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymdp_dev_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
